{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6f1b82",
   "metadata": {},
   "source": [
    "## Optimisation using Newton's Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f781dd96-92fe-486e-b7ba-22a4d7183f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import log\n",
    "import shutil\n",
    "import sys\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c89e27",
   "metadata": {},
   "source": [
    "Given function:\n",
    "\\begin{align*}\n",
    "    f(x_1, x_2, x_3) = x_{3} \\log \\Big( e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}} \\Big) + (x_{3}-2)^2 + e^{\\frac{1}{x_{1} + x_{2}}}\n",
    "\\end{align*}\n",
    "\n",
    "$ \\textbf{dom} \\; f: \\{ \\mathbf{x} \\in \\mathbb{R}^3 : x_1 +x _2 >0, x_3 > 0 \\}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "a00c17ed-e558-411f-a32c-13543edef028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining our function\n",
    "def my_f(x):    \n",
    "    val = x[2] * log(np.exp(x[0] / x[2]) + np.exp(x[1] / x[2])) + (x[2] - 2)**2 + np.exp(1/(x[0] + x[1]))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73fe9d7",
   "metadata": {},
   "source": [
    "Defining the first derivative: \n",
    "\n",
    "$\\nabla f = [ \\partial f/\\partial x_1 \\; \\partial f/\\partial x_2 \\; \\partial f/\\partial x_3]^T   $\n",
    "\n",
    "$$ \\implies \\nabla f = \\begin{Bmatrix}\n",
    "\\frac{e^{\\frac{x_{1}} {x_{3}}}}{e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}} - \\frac{e^{ \\frac{1}{x_1 + x_2}}}{(x_1 +x_2)^2}  \\\\ \\\\\n",
    "\\frac{e^{\\frac{x_{2}} {x_{3}}}}{e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}} - \\frac{e^{ \\frac{1}{x_1 + x_2}}}{(x_1 +x_2)^2} \\\\ \\\\\n",
    " log(e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}) - \\frac{x_1 e^{\\frac{x_{1}} {x_{3}}} + x_2 e^{\\frac{x_{2}} {x_{3}}}}{x_3 ( e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}) } + 2(x_3-2)\n",
    "\\end{Bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "c174c72d-2d61-44bb-81ae-94bd0fe6edd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the first derivative of the function\n",
    "def nabla_f(x):\n",
    "    x1, x2, x3 = x[0], x[1], x[2]\n",
    "    f = np.array([\n",
    "        [np.exp(x1 / x3) / (np.exp(x1 / x3) + np.exp(x2 / x3)) - (1/((x1+x2)**2))*np.exp(1/(x1 + x2))],\n",
    "        [np.exp(x2 / x3) / (np.exp(x1 / x3) + np.exp(x2 / x3)) - (1/((x1+x2)**2))*np.exp(1/(x1 + x2))],\n",
    "        [np.log(np.exp(x1 / x3) + np.exp(x2 / x3)) - (x1 * np.exp(x1 / x3) + x2 * np.exp(x2 / x3)) /\n",
    "         (x3 * (np.exp(x1 / x3) + np.exp(x2 / x3))) + 2 * (x[2] - 2)]\n",
    "    ])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678104f",
   "metadata": {},
   "source": [
    "Defining the Second Derivative:\n",
    "\n",
    "The gradient vector, denoted as $\\nabla f$, is the vector of partial derivatives:\n",
    "$\n",
    "\\nabla f(x) = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right)\n",
    "$\n",
    "\n",
    "The finite difference approximation for the gradient at a point $x$ is given by:\n",
    "$\n",
    "\\nabla_f(x) \\approx \\frac{f(x + h\\mathbf{i}) - f(x)}{h}\n",
    "$\n",
    "where $\\mathbf{i}$ is a unit vector along one of the coordinate axes.\n",
    "\n",
    "The second derivative matrix is then approximated as:\n",
    "$\n",
    "\\nabla^2 f(x) \\approx \\frac{1}{h} \\left(\\nabla_f(x + h\\mathbf{i}) - \\nabla_f(x)\\right)\n",
    "$\n",
    "\n",
    "The reshaped second derivative matrix is a $3 \\times 3$ matrix obtained from the flattened vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d629cdee-5de3-49aa-8b92-0a4cc264a0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defing second derivative of the function\n",
    "def nabla2_f(x):\n",
    "    x = x.flatten()\n",
    "    h = 1e-5  # Step size\n",
    "    identity_matrix = np.eye(len(x))  # Identity matrix\n",
    "\n",
    "    # Construct the perturbation matrix with h values along the diagonal\n",
    "    h_matrix = h * identity_matrix\n",
    "\n",
    "    # Calculate the forward differences for all components simultaneously\n",
    "    perturbed_values = np.array([nabla_f(x + h_vec) for h_vec in h_matrix])\n",
    "\n",
    "    # Calculate the second derivative approximation\n",
    "    second_derivative_matrix = (perturbed_values - nabla_f(x)) / h\n",
    "    \n",
    "    reshaped_second_derivative_matrix = np.reshape(second_derivative_matrix, (3, 3))\n",
    "    \n",
    "    return reshaped_second_derivative_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05316539",
   "metadata": {},
   "source": [
    "Defining parameters for backtracking search and the start point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "6bdfb9ab-92d1-490f-9362-e55685645c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alp = 0.4\n",
    "beta = 0.5\n",
    "eps = 10**(-5)\n",
    "x_start = np.array([3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c120318",
   "metadata": {},
   "source": [
    "Ensuring domain:\n",
    "\n",
    "$ \\text{While} \\; x + t\\Delta x \\notin \\textbf{dom} f, \\text{ set } t := \\beta t $ \\\n",
    "where\n",
    "$\\Delta x = -\\nabla^2 f(x)^{-1} \\nabla f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "30d00127-50fc-4365-92fc-8eec1641dac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensuring Domain\n",
    "def domain_t(x):\n",
    "    t = 1\n",
    "    while True:\n",
    "        invdel2_f = np.linalg.inv(nabla2_f(x))      # invdel2_f.shape = (3, 3), type = numpy.ndarray\n",
    "        del_f = nabla_f(x).flatten()\n",
    "        direction = - np.dot(invdel2_f,del_f.flatten())             # Newton Step\n",
    "        v = x + t * direction\n",
    "        e3 = v[2]\n",
    "        e2 = v[1]\n",
    "        e1 = v[0]\n",
    "\n",
    "        if e3 > 0 and (e2+e1>0):\n",
    "            return t  # Exit the loop and return 't' if the condition is met\n",
    "\n",
    "        # If (e3) or (e1+ e2) is negative , adjust 't' and update 'x'\n",
    "        t *= beta\n",
    "\n",
    "    return None  # Return None if the condition doesn't satisfy within the maximum iterations (which can be defined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efdfd7c",
   "metadata": {},
   "source": [
    "Backtracking algorithm:\n",
    "\n",
    "$\n",
    "\\text{Given a descent direction } \\Delta x = -\\nabla^2 f(x)^{-1} \\nabla f(x) \\text{ for } f \\text{ at } x \\in \\textbf{dom} f, \\alpha \\in (0, 0.5), \\beta \\in (0, 1).$\n",
    "\n",
    "\\begin{array}{l}\n",
    "\\text{Set } t := 1. \\\\ \n",
    "\\text{Ensure domain:} \\; \\text{While} \\; x + t\\Delta x \\notin \\textbf{dom} f, \\text{ set } t := \\beta t \\\\\n",
    "\\text{While } f(x + t\\Delta x) > f(x) + \\alpha t \\nabla f(x)^T \\Delta x, \\text{ set } t := \\beta t.\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "07d80523-e8e6-4511-a38d-6098b55ed64a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Backtracking Algorithm\n",
    "def Backtrack_t(x):\n",
    "    t = domain_t(x)\n",
    "    invdel2_f = np.linalg.inv(nabla2_f(x))                     # invdel2_f.shape = (3, 3), type = numpy.ndarray\n",
    "    del_f = nabla_f(x).flatten()\n",
    "    direction = - np.dot(invdel2_f,del_f.flatten())            # Newton Step\n",
    "    \n",
    "    xv = x + t * direction\n",
    "    le = my_f(xv)                                              # Left expression\n",
    "    \n",
    "    re1 = my_f(x)\n",
    "    decrement = np.dot(del_f.T,direction.flatten())            \n",
    "    re2 = decrement \n",
    "    re = re1 + alp * t * re2                                   # Right expression\n",
    "\n",
    "    while le > re:\n",
    "        t *= beta\n",
    "                \n",
    "        xv = x + t * direction\n",
    "        le = my_f(xv)\n",
    "        re = re1 + alp * t * re2\n",
    "    return t     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b6ffb",
   "metadata": {},
   "source": [
    "### Newton's Algorithm\n",
    "\n",
    "**Input:** (defined earlier)\n",
    "- Starting point $x \\in \\text{dom} \\, f$\n",
    "- Tolerance $\\varepsilon > 0$\n",
    "\n",
    "**Repeat:**\n",
    "1. Compute the Newton step and decrement.\n",
    "   $\n",
    "   \\Delta x_{nt} := -\\nabla^2 f(x)^{-1}\\nabla f(x); \\quad \\lambda^2 := \\nabla f(x)^T \\nabla^2 f(x)^{-1}\\nabla f(x)\n",
    "   $\n",
    "2. Stopping criterion. $\\textbf{quit}$ if $\\lambda^2/2 \\leq \\varepsilon.$\n",
    "3. Line search. Choose step size $t$ by backtracking line search; ensuring update $ := x + t \\Delta x$ lies in $\\mathbf{dom} f$ throughout.\n",
    "4. Update. $x := x + t\\Delta x_{nt}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d204ac24-15e8-4bd9-b142-918c70d1bf26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [0.9259803  0.92600427 1.65330163]\n",
      "Optimal function value: 3.9081138659666284\n",
      "Number of iterations taken to converge: 4\n"
     ]
    }
   ],
   "source": [
    "# Running Newton's Algorithm\n",
    "iter =0\n",
    "while True:\n",
    "    invdel2_f = np.linalg.inv(nabla2_f(x_start))            # invdel2_f.shape = (3, 3), type = numpy.ndarray\n",
    "    del_f = nabla_f(x_start).flatten()\n",
    "    direction = - np.dot(invdel2_f,del_f.flatten())         # Newton Step\n",
    "    \n",
    "    decrement = - np.dot(del_f.T,direction.flatten())       # lambda^2 condition\n",
    "    \n",
    "    if (decrement /2 <= eps):\n",
    "        break\n",
    "    \n",
    "    t = Backtrack_t(x_start)                                 # Choosing t using Line Search\n",
    "    \n",
    "    x_start = x_start + t * direction                        # Update Step\n",
    "    iter=iter+1\n",
    "    \n",
    "print(\"Optimal solution:\", x_start)\n",
    "fopt = my_f(x_start)\n",
    "print(\"Optimal function value:\", fopt)\n",
    "print(\"Number of iterations taken to converge:\", iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
