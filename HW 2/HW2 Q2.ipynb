{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b88ee7",
   "metadata": {},
   "source": [
    "## Optimisation using Quasi-Newton Method:\n",
    "### The Broyden-Fletcher-Goldfarb-Shanno (BFGS) update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f0c2542b-c181-4258-b882-948ddbcefcb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import log\n",
    "import shutil\n",
    "import sys\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec9b5b",
   "metadata": {},
   "source": [
    "Given function:\n",
    "\\begin{align*}\n",
    "    f(x_1, x_2, x_3) = x_{3} \\log \\Big( e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}} \\Big) + (x_{3}-2)^2 + e^{\\frac{1}{x_{1} + x_{2}}}\n",
    "\\end{align*}\n",
    "\n",
    "$ \\textbf{dom} \\; f: \\{ \\mathbf{x} \\in \\mathbb{R}^3 : x_1 +x _2 >0, x_3 > 0 \\}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0a40ec31-87b2-4efa-92e5-524f97d79c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining our function\n",
    "def my_f(x):    \n",
    "    val = x[2] * log(np.exp(x[0] / x[2]) + np.exp(x[1] / x[2])) + (x[2] - 2)**2 + np.exp(1/(x[0] + x[1]))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af12e8",
   "metadata": {},
   "source": [
    "Defining the first derivative: \n",
    "\n",
    "$\\nabla f = [ \\partial f/\\partial x_1 \\; \\partial f/\\partial x_2 \\; \\partial f/\\partial x_3]^T   $\n",
    "\n",
    "$$ \\implies \\nabla f = \\begin{Bmatrix}\n",
    "\\frac{e^{\\frac{x_{1}} {x_{3}}}}{e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}} - \\frac{e^{ \\frac{1}{x_1 + x_2}}}{(x_1 +x_2)^2}  \\\\ \\\\\n",
    "\\frac{e^{\\frac{x_{2}} {x_{3}}}}{e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}} - \\frac{e^{ \\frac{1}{x_1 + x_2}}}{(x_1 +x_2)^2} \\\\ \\\\\n",
    " log(e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}) - \\frac{x_1 e^{\\frac{x_{1}} {x_{3}}} + x_2 e^{\\frac{x_{2}} {x_{3}}}}{x_3 ( e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}) } + 2(x_3-2)\n",
    "\\end{Bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "100fc892-c305-4c22-81df-bea4b4e8b81e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the first derivative of the function\n",
    "def nabla_f(x):\n",
    "    x1, x2, x3 = x[0], x[1], x[2]\n",
    "    f = np.array([\n",
    "        [np.exp(x1 / x3) / (np.exp(x1 / x3) + np.exp(x2 / x3)) - (1/((x1+x2)**2))*np.exp(1/(x1 + x2))],\n",
    "        [np.exp(x2 / x3) / (np.exp(x1 / x3) + np.exp(x2 / x3)) - (1/((x1+x2)**2))*np.exp(1/(x1 + x2))],\n",
    "        [np.log(np.exp(x1 / x3) + np.exp(x2 / x3)) - (x1 * np.exp(x1 / x3) + x2 * np.exp(x2 / x3)) /\n",
    "         (x3 * (np.exp(x1 / x3) + np.exp(x2 / x3))) + 2 * (x[2] - 2)]\n",
    "    ])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684ac86",
   "metadata": {},
   "source": [
    "Defining the Second Derivative:\n",
    "\n",
    "The gradient vector, denoted as $\\nabla f$, is the vector of partial derivatives:\n",
    "$\n",
    "\\nabla f(x) = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right)\n",
    "$\n",
    "\n",
    "The finite difference approximation for the gradient at a point $x$ is given by:\n",
    "$\n",
    "\\nabla_f(x) \\approx \\frac{f(x + h\\mathbf{i}) - f(x)}{h}\n",
    "$\n",
    "where $\\mathbf{i}$ is a unit vector along one of the coordinate axes.\n",
    "\n",
    "The second derivative matrix is then approximated as:\n",
    "$\n",
    "\\nabla^2 f(x) \\approx \\frac{1}{h} \\left(\\nabla_f(x + h\\mathbf{i}) - \\nabla_f(x)\\right)\n",
    "$\n",
    "\n",
    "The reshaped second derivative matrix is a $3 \\times 3$ matrix obtained from the flattened vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "31f81de6-ac49-4813-a227-badf334c4e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defing second derivative of the function\n",
    "def nabla2_f(x):\n",
    "    x = x.flatten()\n",
    "    h = 1e-2  # Step size\n",
    "    identity_matrix = np.eye(len(x))  # Identity matrix\n",
    "\n",
    "    # Construct the perturbation matrix with h values along the diagonal\n",
    "    h_matrix = h * identity_matrix\n",
    "\n",
    "    # Calculate the forward differences for all components simultaneously\n",
    "    perturbed_values = np.array([nabla_f(x + h_vec) for h_vec in h_matrix])\n",
    "\n",
    "    # Calculate the second derivative approximation\n",
    "    second_derivative_matrix = (perturbed_values - nabla_f(x)) / h\n",
    "    \n",
    "    reshaped_second_derivative_matrix = np.reshape(second_derivative_matrix, (3, 3))\n",
    "    \n",
    "    return reshaped_second_derivative_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f668fb",
   "metadata": {},
   "source": [
    "Defining parameters for backtracking search and the start point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2af3379b-e2f6-4d13-8adb-90e53a3b398f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alp = 0.4\n",
    "beta = 0.5\n",
    "eps = 10**(-5)\n",
    "x_start = np.array([2,3,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdbb6c",
   "metadata": {},
   "source": [
    "Ensuring domain:\n",
    "\n",
    "$ \\text{While} \\; x + t\\Delta x \\notin \\textbf{dom} f, \\text{ set } t := \\beta t $ \n",
    "\n",
    "Here, the direction $\\Delta x$ has been taken as a parameter in the defined function and thus we need not explicitly calculate it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "196bbca7-3bf8-4712-ace7-0980c62651f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensuring Domain\n",
    "def domain_t(x,direction):\n",
    "    t = 1\n",
    "    while True:\n",
    "        v = x + t * direction\n",
    "        e3 = v[2]\n",
    "        e2 = v[1]\n",
    "        e1 = v[0]\n",
    "\n",
    "        if e3 > 0 and (e2+e1>0):\n",
    "            return t  # Exit the loop and return 't' if the condition is met\n",
    "\n",
    "        # If (e3) or (e1+ e2) is negative , adjust 't' and update 'x'\n",
    "        t *= beta\n",
    "\n",
    "    return None  # Return None if the condition doesn't satisfy within the maximum iterations (which can be defined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7366aaff",
   "metadata": {},
   "source": [
    "Backtracking algorithm:\n",
    "\n",
    "$\n",
    "\\text{Given a descent direction } \\Delta x  \\text{ for } f \\text{ at } x \\in \\textbf{dom} f, \\alpha \\in (0, 0.5), \\beta \\in (0, 1).$\n",
    "\n",
    "\\begin{array}{l}\n",
    "\\text{Set } t := 1. \\\\ \n",
    "\\text{Ensure domain:} \\; \\text{While} \\; x + t\\Delta x \\notin \\textbf{dom} f, \\text{ set } t := \\beta t \\\\\n",
    "\\text{While } f(x + t\\Delta x) > f(x) + \\alpha t \\nabla f(x)^T \\Delta x, \\text{ set } t := \\beta t.\n",
    "\\end{array}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "113a183c-9cb7-4979-a6ed-6f626d9326db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Backtracking Algorithm\n",
    "def Backtrack_t(x,direction):\n",
    "    t = domain_t(x,direction)\n",
    "    del_f = nabla_f(x).flatten()\n",
    "    \n",
    "    xv = x + t * direction\n",
    "    le = my_f(xv)                                              # Left expression \n",
    "    re1 = my_f(x)\n",
    "    re2 = np.dot(del_f.T,direction.flatten())           \n",
    "    re = re1 + alp * t * re2                                   # Right expression\n",
    "\n",
    "    while le > re:\n",
    "        t *= beta\n",
    "        \n",
    "        xv = x + t * direction\n",
    "        le = my_f(xv)          \n",
    "        re = re1 + alp * t * re2\n",
    "    return t     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba454a",
   "metadata": {},
   "source": [
    "### Running the BFGS Algorithm: \n",
    "\n",
    "**Given:**\n",
    "- Starting point $x_0$\n",
    "- Convergence tolerance $\\epsilon > 0$ $(=10^{-5})$\n",
    "- Starting matrix $H_0$ (taken as Identity matrix)\n",
    "  \n",
    "**Initialization:** $k \\gets 0$\n",
    "\n",
    "**While:** $\\| \\nabla f_k \\| > \\epsilon$\n",
    "1. Compute search direction by solving:\n",
    "   $ p_k = - H_k \\nabla f_k $\n",
    "2. Set $x_{k+1} = x_k + \\alpha_k p_k$, where $\\alpha_k$ is computed from backtracking line search procedure.\n",
    "3. Define $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f_{k+1} - \\nabla f_k$.\n",
    "4. Compute $H_{k+1}$ using BFGS (as given in the tutorial).\n",
    "5. $k \\gets k + 1$\n",
    "\n",
    "Finally, as mentioned in the implementation note to set $\\rho_k $ as a constant after $y^T_k s_k$ gets smaller some certain $\\epsilon$ say $10^{-5}$.\\\n",
    "We have used an $\\epsilon = 10^{-4}$ and set the value for  $\\rho_k = 10^{4}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5443211d-0787-4ae1-a5d8-633a7cceabce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [0.92619643 0.92622045 1.65342624]\n",
      "Optimal function value: 3.908113786305548\n",
      "Number of iterations taken to converge: 183\n"
     ]
    }
   ],
   "source": [
    "# Running BFGS Algorithm\n",
    "iter =0\n",
    "H_start = np.eye(3)                             # Inverse Hessian aprroxmiation as Identity\n",
    "norm_nabla_f = np.dot(nabla_f(x_start).flatten(), nabla_f(x_start).flatten())**0.5\n",
    "\n",
    "while norm_nabla_f > eps:\n",
    "    \n",
    "    # Computing Search Direction        (By Avoiding explicit matrix inversion for numerical stability and efficiency)\n",
    "    del_f = nabla_f(x_start).flatten()\n",
    "    direction = - np.dot(H_start,del_f)                        # Finding Direction\n",
    "    \n",
    "    t = Backtrack_t(x_start,direction)                         # Choosing t using Line Search\n",
    "    \n",
    "    x_new = x_start + t * direction       # Update Step\n",
    "    \n",
    "    s = x_new - x_start                  \n",
    "    y = nabla_f(x_new).flatten() - del_f\n",
    "    \n",
    "    p = np.dot(y.T,s)   # Finding p\n",
    "    if p<1e-4:\n",
    "        p = 1e-4\n",
    "    p = 1/p            # 1/p\n",
    "    \n",
    "    t1 = np.eye(3) - p*np.dot(s,y.T)\n",
    "    t2 = p*np.dot(s,s.T)\n",
    "    \n",
    "    m1 = np.dot(t1,H_start)\n",
    "    t1 = np.dot(m1,t1) \n",
    "\n",
    "    H_new = t1 + t2     # Calculating H_(k+1)\n",
    "    \n",
    "    H_start = H_new\n",
    "    x_start = x_new\n",
    "    iter=iter+1        # Iteration counter\n",
    "    norm_nabla_f = np.dot(nabla_f(x_start).flatten(), nabla_f(x_start).flatten())**0.5\n",
    "    \n",
    "print(\"Optimal solution:\", x_start)\n",
    "fopt = my_f(x_start)\n",
    "print(\"Optimal function value:\", fopt)\n",
    "print(\"Number of iterations taken to converge:\", iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
