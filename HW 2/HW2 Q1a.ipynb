{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f745aaa9",
   "metadata": {},
   "source": [
    "##  Gradient Descent with Backtracking Line Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d4ae89c9-5122-4e0c-80b9-e0973369c46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import log\n",
    "import shutil\n",
    "import sys\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b872d",
   "metadata": {},
   "source": [
    "Given function:\n",
    "\\begin{align*}\n",
    "    f(x_1, x_2, x_3) = x_{3} \\log \\Big( e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}} \\Big) + (x_{3}-2)^2 + e^{\\frac{1}{x_{1} + x_{2}}}\n",
    "\\end{align*}\n",
    "\n",
    "$ \\textbf{dom} \\; f: \\{ \\mathbf{x} \\in \\mathbb{R}^3 : x_1 +x _2 >0, x_3 > 0 \\}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6221d128-d77c-4cca-8ac4-9b32b83a2255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining our function\n",
    "def my_f(x):    \n",
    "    val = x[2] * log(np.exp(x[0] / x[2]) + np.exp(x[1] / x[2])) + (x[2] - 2)**2 + np.exp(1/(x[0] + x[1]))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c8d40",
   "metadata": {},
   "source": [
    "Defining the first derivative: \n",
    "\n",
    "$\\nabla f = [ \\partial f/\\partial x_1 \\; \\partial f/\\partial x_2 \\; \\partial f/\\partial x_3]^T   $\n",
    "\n",
    "$$ \\implies \\nabla f = \\begin{Bmatrix}\n",
    "\\frac{e^{\\frac{x_{1}} {x_{3}}}}{e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}} - \\frac{e^{ \\frac{1}{x_1 + x_2}}}{(x_1 +x_2)^2}  \\\\ \\\\\n",
    "\\frac{e^{\\frac{x_{2}} {x_{3}}}}{e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}} - \\frac{e^{ \\frac{1}{x_1 + x_2}}}{(x_1 +x_2)^2} \\\\ \\\\\n",
    " log(e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}) - \\frac{x_1 e^{\\frac{x_{1}} {x_{3}}} + x_2 e^{\\frac{x_{2}} {x_{3}}}}{x_3 ( e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}) } + 2(x_3-2)\n",
    "\\end{Bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "117db820-5562-4313-af26-846be09c006b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the first derivative of the function\n",
    "def nabla_f(x):\n",
    "    x1, x2, x3 = x[0], x[1], x[2]\n",
    "    f = np.array([\n",
    "        [np.exp(x1 / x3) / (np.exp(x1 / x3) + np.exp(x2 / x3)) - (1/((x1+x2)**2))*np.exp(1/(x1 + x2))],\n",
    "        [np.exp(x2 / x3) / (np.exp(x1 / x3) + np.exp(x2 / x3)) - (1/((x1+x2)**2))*np.exp(1/(x1 + x2))],\n",
    "        [np.log(np.exp(x1 / x3) + np.exp(x2 / x3)) - (x1 * np.exp(x1 / x3) + x2 * np.exp(x2 / x3)) /\n",
    "         (x3 * (np.exp(x1 / x3) + np.exp(x2 / x3))) + 2 * (x[2] - 2)]\n",
    "    ])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68db1dc",
   "metadata": {},
   "source": [
    "Defining parameters for backtracking search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "6ee422e8-763c-4063-806a-b1c5b380e7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alp = 0.4\n",
    "beta = 0.5\n",
    "eps = 10**(-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d7359",
   "metadata": {},
   "source": [
    "Start Point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "60de5b5e-4a39-4740-be88-5c896dd2217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_start = np.array([3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945f22b",
   "metadata": {},
   "source": [
    "Ensuring domain:\n",
    "\n",
    "$ \\text{While} \\; x + t\\Delta x \\notin \\textbf{dom} f, \\text{ set } t := \\beta t $ \\\n",
    "where, $\\Delta x = -\\nabla f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1d046b67-798c-405e-9ca5-8d605d1b4cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensuring Domain\n",
    "def domain_t(x):\n",
    "    t = 1\n",
    "    while True:\n",
    "        v = x - t * nabla_f(x).flatten()\n",
    "        e3 = v[2]\n",
    "        e2 = v[1]\n",
    "        e1 = v[0]\n",
    "\n",
    "        if e3 > 0 and (e2+e1>0):\n",
    "            return t  # Exit the loop and return 't' if the condition is met\n",
    "\n",
    "        # If (e3) or (e1+ e2) is negative , adjust 't' and update 'x'\n",
    "        t *= beta\n",
    "\n",
    "    return None  # Return None if the condition doesn't satisfy within the maximum iterations (which can be defined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96676d20",
   "metadata": {},
   "source": [
    "Backtracking algorithm:\n",
    "\n",
    "$\n",
    "\\text{Given a descent direction } \\Delta x = -\\nabla f(x) \\text{ for } f \\text{ at } x \\in \\textbf{dom} f, \\alpha \\in (0, 0.5), \\beta \\in (0, 1).$\n",
    "\n",
    "\\begin{array}{l}\n",
    "\\text{Set } t := 1. \\\\ \n",
    "\\text{Ensure domain:} \\; \\text{While} \\; x + t\\Delta x \\notin \\textbf{dom} f, \\text{ set } t := \\beta t \\\\\n",
    "\\text{While } f(x + t\\Delta x) > f(x) + \\alpha t \\nabla f(x)^T \\Delta x, \\text{ set } t := \\beta t.\n",
    "\\end{array}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "6e29ee07-192b-4af4-a8cb-0e76cfaef980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Backtracking Algorithm\n",
    "def Backtrack_t(x):\n",
    "    t = domain_t(x)\n",
    "    xv = x - t * nabla_f(x).flatten()\n",
    "    le = my_f(xv)                                              # Left expression\n",
    "    re1 = my_f(x)\n",
    "    re2 = np.dot(nabla_f(x).flatten(), nabla_f(x).flatten())\n",
    "    re = re1 - alp * t * re2                                   # Right expression\n",
    "\n",
    "    while le > re:\n",
    "        t *= beta\n",
    "        xv = x - t * nabla_f(x).flatten()\n",
    "        le = my_f(xv)\n",
    "        re = re1 - alp * t * re2\n",
    "    return t     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3246df8",
   "metadata": {},
   "source": [
    "### Algorithm: Gradient Descent\n",
    "\n",
    "1. **Input:** Starting point $x$ in $\\text{dom} \\, f$\n",
    "\n",
    "2. **Repeat until stopping criterion is satisfied:**\n",
    "\n",
    "    a. $\\Delta x := -\\nabla f(x)$\n",
    "    \n",
    "    b. **Line search:** Choose step size $t$ via backtracking line search\n",
    "    \n",
    "    c. **Update:** $x := x + t \\Delta x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "94c099d9-a67d-46fd-b3ae-aa753606c93d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [0.92618727 0.92622965 1.65342641]\n",
      "Optimal function value: 3.908113786397637\n",
      "Number of iterations taken to converge: 30\n"
     ]
    }
   ],
   "source": [
    "# Running Gradient Descent with Backtracking Line Search\n",
    "norm_nabla_f = np.dot(nabla_f(x_start).flatten(), nabla_f(x_start).flatten())**0.5\n",
    "\n",
    "iter =0\n",
    "while norm_nabla_f > eps:\n",
    "    direction = -nabla_f(x_start).flatten()\n",
    "    t = Backtrack_t(x_start)\n",
    "    x_start = x_start + t * direction\n",
    "    norm_nabla_f = np.dot(nabla_f(x_start).flatten(), nabla_f(x_start).flatten())**0.5\n",
    "    iter=iter+1\n",
    "\n",
    "print(\"Optimal solution:\", x_start)\n",
    "fopt = my_f(x_start)\n",
    "print(\"Optimal function value:\", fopt)\n",
    "print(\"Number of iterations taken to converge:\", iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
